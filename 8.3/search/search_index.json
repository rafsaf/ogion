{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Ogion","text":""},{"location":"#ogion","title":"Ogion","text":"<p>A tool for performing scheduled database backups and transferring encrypted data to secure public clouds, for home labs, hobby projects, etc., in environments such as k8s, docker, vms.</p> <p>Backups are in <code>age</code> format using age, with strong encryption under the hood. Why age? it's modern replacement for GnuPG, available for most architectures and systems.</p> <p>This project is more or less well tested cron-like runtime with predefined supported providers and backup targets (see below) with sensible defaults for backup commands. It has rich integration tests using providers container replacements: fake gcs, azurite, minio. Goal was to make 100% sure it will work in the wild.</p> <p>Starting from version 8.0, lzip compression is used before encryption step. While mixing compression with encryption can be dangerous in some scenarios, <code>lzip</code> is used here, because it operates on fixed-size blocks, making it resistant to compression side-channel attacks.</p>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>https://ogion.rafsaf.pl</li> </ul>"},{"location":"#alternatives","title":"Alternatives","text":"<p>There are better tools for bigger databases like pgBackRest - Reliable PostgreSQL Backup &amp; Restore.</p>"},{"location":"#supported-backup-targets","title":"Supported backup targets","text":"<ul> <li>PostgreSQL (all currently supported versions)</li> <li>MariaDB (all currently supported versions)</li> <li>MySQL (all currently supported versions)</li> <li>Single file</li> <li>Directory</li> </ul>"},{"location":"#supported-upload-providers","title":"Supported upload providers","text":"<ul> <li>Google Cloud Storage bucket</li> <li>S3 storage compatibile bucket (AWS, Minio)</li> <li>Azure Blob Storage</li> <li>Debug (local)</li> </ul>"},{"location":"#notifications","title":"Notifications","text":"<ul> <li>Discord</li> <li>Email (SMTP)</li> <li>Slack</li> </ul>"},{"location":"#deployment-strategies","title":"Deployment strategies","text":"<p>Using docker image: <code>rafsaf/ogion:latest</code>, see all tags on dockerhub</p> <ul> <li>docker (docker compose) container</li> <li>kubernetes deployment</li> </ul>"},{"location":"#architectures","title":"Architectures","text":"<ul> <li>linux/amd64</li> <li>linux/arm64</li> </ul>"},{"location":"#example","title":"Example","text":"<p>Everyday 5am backup of PostgreSQL database defined in the same file and running in docker container.</p> <pre><code># docker-compose.yml\n\nservices:\n  db:\n    image: postgres:17\n    environment:\n      - POSTGRES_PASSWORD=pwd\n  ogion:\n    image: rafsaf/ogion:latest\n    environment:\n      - POSTGRESQL_DB_README=host=db password=pwd cron_rule=0 0 5 * * port=5432\n      - AGE_RECIPIENTS=age1q5g88krfjgty48thtctz22h5ja85grufdm0jly3wll6pr9f30qsszmxzm2\n      - BACKUP_PROVIDER=name=debug\n</code></pre> <p>(NOTE this will use provider debug that store backups locally in the container).</p>"},{"location":"#real-world-usage","title":"Real world usage","text":"<p>The author actively uses ogion (with GCS) for one production project plemiona-planer.pl postgres database (both PRD and STG) and for bunch of homelab projects including self hosted Firefly III mariadb, Grafana postgres, KeyCloak postgres, Nextcloud postgres and configuration file, Minecraft server files, and two other postgres dbs for some demo projects.</p> <p>See how it looks for ~2GB size database:</p> <p></p>"},{"location":"#why-it-works","title":"Why it works","text":"<p>Ogion has 100% test coverage with end-to-end integration tests that guarantee it works in real scenarios. Tests run inside containers on all architectures against actual database instances (PostgreSQL 13-18, MariaDB 10.6-12.0, MySQL 8.0-9.5) defined in <code>docker-compose.dbs.yml</code>. Database versions are automatically updated daily via GitHub Actions to ensure compatibility with latest releases.</p> <p>Each test executes the complete backup/restore cycle:</p> <ol> <li>Create test data in real database</li> <li>Run backup using actual database clients (<code>pg_dump</code>, <code>mariadb-dump</code>)</li> <li>Compress with lzip and encrypt with age</li> <li>Upload to simulated cloud storage (Azurite for Azure, fake-gcs-server for GCS, Minio for S3)</li> <li>Download the backup</li> <li>Decrypt with age and decompress with lzip</li> <li>Restore to database and verify data integrity</li> </ol> <p>This end-to-end testing against real databases and cloud storage simulators ensures ogion will work reliably in production.</p> <p> </p>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#unreleased","title":"[Unreleased]","text":""},{"location":"changelog/#added","title":"Added","text":""},{"location":"changelog/#changed","title":"Changed","text":""},{"location":"changelog/#fixed","title":"Fixed","text":""},{"location":"changelog/#83-2025-11-12","title":"[8.3] - 2025-11-12","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Shell autocomplete support - Added bash autocomplete for all CLI commands and arguments using <code>argcomplete</code> library. Autocomplete works for target names, backup files, and all command options</li> <li>New <code>ogion</code> command (default entrypoint) - Added <code>ogion</code> bash script as a convenient shortcut for <code>python -m ogion.main</code>. Both commands work identically</li> <li>Improved <code>--single</code> flag - Can now backup a specific target using <code>--single --target &lt;name&gt;</code> instead of running all backups</li> <li><code>BACKUP_DELETE</code> environment variable - New boolean configuration option (default: <code>true</code>) to control automatic cleanup operations.</li> <li>GitHub Copilot instructions</li> <li>Added <code>--debug-loop</code> command and <code>mem_stress_test</code> CI/CD</li> </ul>"},{"location":"changelog/#changed_1","title":"Changed","text":"<ul> <li>Migrated from Poetry to uv for dependency management</li> <li>Added this Changelog file</li> <li>Migrate to Python 3.14 and Debian Trixie</li> <li>Better argument validation - CLI now validates argument combinations upfront and shows clear error messages</li> <li>Performance: removed verbose flags (<code>-v</code>) from archive/DB tools to cut excessive stdout and logging overhead</li> <li>Performance: debug provider downloads now stream in chunks instead of reading entire files into memory</li> <li><code>LZIP_THREADS</code> default behavior - Changed from <code>1</code> to <code>null</code> which translate to automatic CPU detection</li> </ul>"},{"location":"changelog/#fixed_1","title":"Fixed","text":"<ul> <li>Memory leaks for (GCS, S3, Azure) upload providers - objects are now cached and reused across threads, preventing memory growth from unclosed client connections during long-running operations. Currently all provider clients are thread-safe. </li> <li>Edge case fixes for <code>--list</code> and <code>--restore-latest</code> commands when using many similar env names for azure and gcs (eg. POSTGRESQL_TEST, POSTGRESQL_TEST_HOURLY could lead to use of wrong backup file name in <code>--restore-latest</code> and wrong list in <code>--list</code>)</li> <li>Concurrent backup clean race condition (local) - Previously, the <code>clean()</code> method would delete all files in the source backup directory using <code>iterdir()</code>, causing failures when multiple backup threads ran simultaneously. Now, <code>post_save()</code> immediately removes local files after upload, and <code>clean()</code> only handles remote storage cleanup. This prevents threads from deleting each other's in-progress backup files</li> <li>Concurrent backup clean race condition (remote) - Fixed crash when multiple backups run simultaneously and try to delete the same old backup file. GCS, Azure, and S3 providers now gracefully handle \"file not found\" errors during cleanup, treating them as success</li> <li>Missing cleanup of intermediate .lz files - Now <code>run_create_age_archive()</code> properly calls <code>remove_path()</code> to delete the intermediate compressed file after creating the <code>.lz.age</code> archive instead of this being side effect of <code>clean()</code></li> <li>TOCTOU race condition in file deletion - Fixed Time-of-Check-Time-of-Use issue in <code>remove_path()</code> could cause errors in concurrent scenarios</li> <li>Use <code>recursive=True</code> in minio client <code>list_objects</code> method. In mem stress test concurrent scenario, it was possible to get object with incorrect name with <code>/</code> at the end.</li> </ul>"},{"location":"changelog/#82-2025-10-05","title":"[8.2] - 2025-10-05","text":""},{"location":"changelog/#added_2","title":"Added","text":"<ul> <li>PostgreSQL client 18 support</li> <li>Use <code>getpass</code> for password prompts (improved security)</li> </ul>"},{"location":"changelog/#fixed_2","title":"Fixed","text":"<ul> <li>Memory leak in croniter</li> <li>MariaDB RC images handling in compose file generator</li> </ul>"},{"location":"changelog/#81-2025-07-25","title":"[8.1] - 2025-07-25","text":""},{"location":"changelog/#added_3","title":"Added","text":"<ul> <li>Retry on network errors for upload providers</li> </ul>"},{"location":"changelog/#changed_2","title":"Changed","text":"<ul> <li>Frozen Ruff and mypy versions</li> <li>Don't use patch version of database images in file generator</li> </ul>"},{"location":"changelog/#80-2025-05-13","title":"[8.0] - 2025-05-13","text":""},{"location":"changelog/#added_4","title":"Added","text":"<ul> <li>lzip compression before encryption - all backups now use lzip compression before age encryption for better compression ratios and resistance to compression side-channel attacks</li> <li>Print available backup targets on startup</li> </ul>"},{"location":"changelog/#changed_3","title":"Changed","text":"<ul> <li>BREAKING: Backup format changed - now uses lzip compression layer before age encryption</li> </ul>"},{"location":"changelog/#73-2025-03-01","title":"[7.3] - 2025-03-01","text":""},{"location":"changelog/#added_5","title":"Added","text":"<ul> <li>Benchmark commands for testing</li> <li>Poetry v2 support</li> </ul>"},{"location":"changelog/#fixed_3","title":"Fixed","text":"<ul> <li>Hyphen in extra options parsing bug</li> </ul>"},{"location":"changelog/#72-2024-12-15","title":"[7.2] - 2024-12-15","text":""},{"location":"changelog/#added_6","title":"Added","text":"<ul> <li>Optional parameters support for PostgreSQL backups</li> <li>Optional parameters support for MariaDB backups</li> <li>Lazy loading for upload providers (faster startup)</li> </ul>"},{"location":"changelog/#changed_4","title":"Changed","text":"<ul> <li>Improved documentation</li> </ul>"},{"location":"changelog/#71-2024-10-26","title":"[7.1] - 2024-10-26","text":""},{"location":"changelog/#fixed_4","title":"Fixed","text":"<ul> <li>Google Cloud credentials authentication problem</li> </ul>"},{"location":"changelog/#70-2024-10-25","title":"[7.0] - 2024-10-25","text":""},{"location":"changelog/#added_7","title":"Added","text":"<ul> <li>age encryption replaces 7zip - migration from GPG/7zip to modern age encryption</li> <li>Azure Blob Storage testing with Azurite</li> <li>Minio client for S3 testing</li> <li>Fake GCS server for testing</li> <li>Migrate to python 3.13</li> <li>Database restore commands (<code>restore</code> and <code>restore-latest</code>)</li> <li><code>download</code> and <code>list</code> commands for backups</li> <li><code>all_target_backups</code> functionality</li> </ul>"},{"location":"changelog/#changed_5","title":"Changed","text":"<ul> <li>BREAKING: Encryption format changed from 7zip to age</li> <li>BREAKING: Removed MySQL 8.0 duplicate of MariaDB</li> <li>BREAKING: Environment variable changes for backup targets</li> <li>Removed boto3, now using minio client for S3</li> <li>Improved test coverage to 100%</li> <li>Docker entrypoint script removed, no root mode by default</li> <li>Better SIGTERM handling</li> </ul>"},{"location":"changelog/#removed","title":"Removed","text":"<ul> <li>7zip encryption support (replaced by age)</li> <li>Root mode in Docker by default</li> </ul>"},{"location":"changelog/#60-2024-03-15","title":"[6.0] - 2024-03-15","text":""},{"location":"changelog/#added_8","title":"Added","text":"<ul> <li>End-to-end tests for MySQL, MariaDB, and PostgreSQL dump/restore</li> <li>File and folder backup target tests</li> <li>Test coverage improvements</li> <li>Release automation workflow</li> <li>CONTRIBUTING.md and PR templates</li> </ul>"},{"location":"changelog/#changed_6","title":"Changed","text":"<ul> <li>BREAKING: Project renamed from \"backuper\" to \"Ogion\"</li> <li>BREAKING: Folder renamed from <code>backuper</code> to <code>ogion</code></li> <li>BREAKING: Changed to GNU GPLv3 license</li> <li>Fully migrated to Ruff (removed flake8, black, isort)</li> <li>Pydantic models now used for initialization of backup targets and providers</li> <li>Python 3.12 minimum required version</li> <li>88 character line length enforced</li> </ul>"},{"location":"changelog/#fixed_5","title":"Fixed","text":"<ul> <li>Escape special characters in folder and file names during backup</li> </ul>"},{"location":"changelog/#51-2023-10-15","title":"[5.1] - 2023-10-15","text":""},{"location":"changelog/#added_9","title":"Added","text":"<ul> <li>Automatic docker-compose.dbs.yml updates workflow</li> <li>PostgreSQL 16 support</li> <li>MySQL 8.1 and MariaDB 11.1 support</li> <li>Python 3.12 support</li> </ul>"},{"location":"changelog/#changed_7","title":"Changed","text":"<ul> <li>Renamed MySQL references to MariaDB internally</li> <li>Refactored docker-compose structure</li> </ul>"},{"location":"changelog/#fixed_6","title":"Fixed","text":"<ul> <li>Acceptance tests issues</li> </ul>"},{"location":"changelog/#50-2023-09-15","title":"[5.0] - 2023-09-15","text":""},{"location":"changelog/#added_10","title":"Added","text":"<ul> <li>Slack notifications support</li> <li>SMTP/Email notifications support</li> </ul>"},{"location":"changelog/#changed_8","title":"Changed","text":"<ul> <li>Improved notification system architecture</li> </ul>"},{"location":"changelog/#40-2023-08-27","title":"[4.0] - 2023-08-27","text":""},{"location":"changelog/#added_11","title":"Added","text":"<ul> <li>Azure Blob Storage upload provider</li> <li>Minimum retention days parameter for providers (with 0 days support)</li> <li><code>--clean</code> flag for PostgreSQL backup options</li> <li>Pydantic-based environment variable validation</li> <li>pytest-xdist for faster test execution</li> </ul>"},{"location":"changelog/#changed_9","title":"Changed","text":"<ul> <li>AWS and GCP providers now use SecretStr for sensitive data</li> <li>Updated documentation for all providers</li> </ul>"},{"location":"changelog/#32-2023-08-16","title":"[3.2] - 2023-08-16","text":""},{"location":"changelog/#added_12","title":"Added","text":"<ul> <li>AWS S3 upload provider with boto3</li> <li>Tests for AWS provider</li> </ul>"},{"location":"changelog/#31-2023-08-12","title":"[3.1] - 2023-08-12","text":""},{"location":"changelog/#added_13","title":"Added","text":"<ul> <li>Documentation versioning with mike</li> <li>Docs published at versioned URLs</li> </ul>"},{"location":"changelog/#changed_10","title":"Changed","text":"<ul> <li>Updated configuration documentation links</li> </ul>"},{"location":"changelog/#30-2023-08-12","title":"[3.0] - 2023-08-12","text":""},{"location":"changelog/#added_14","title":"Added","text":"<ul> <li>Two additional options for GCS provider</li> </ul>"},{"location":"changelog/#changed_11","title":"Changed","text":"<ul> <li>BREAKING: Renamed upload provider names</li> <li>Improved GCS provider logic</li> <li>Fixed max_backups handling</li> <li>Better cleanup in run_backup</li> <li>Improved documentation for providers and targets</li> </ul>"},{"location":"changelog/#20-2023-08-08","title":"[2.0] - 2023-08-08","text":""},{"location":"changelog/#added_15","title":"Added","text":"<ul> <li>Thread name to logging output</li> <li>Better logging in backup runs</li> </ul>"},{"location":"changelog/#changed_12","title":"Changed","text":"<ul> <li>BREAKING: Refactored notifications system</li> <li>BREAKING: Renamed storage providers to just \"providers\"</li> <li>BREAKING: Config refactored with better class target model names</li> <li>BREAKING: Environment variable names changed</li> <li>Pydantic 2.0 migration</li> <li>Move to abstract config for backup target detection</li> </ul>"},{"location":"changelog/#removed_1","title":"Removed","text":"<ul> <li>Raw requirements.txt files (poetry only)</li> </ul>"},{"location":"changelog/#12-2023-07-11","title":"[1.2] - 2023-07-11","text":""},{"location":"changelog/#added_16","title":"Added","text":"<ul> <li>Mypy type checking with 100% coverage</li> <li>Bandit security checks</li> <li>Makefile with coverage requirements (100%)</li> </ul>"},{"location":"changelog/#changed_13","title":"Changed","text":"<ul> <li>Migrated to Pydantic 2.0</li> <li>Migrated 7-Zip to version 23.01</li> <li>Python base image to Bookworm</li> <li>Provider architecture now uses <code>__init_subclass__</code></li> </ul>"},{"location":"changelog/#fixed_7","title":"Fixed","text":"<ul> <li>GCS tests storage mock</li> <li>Test timeouts increased to 5s</li> </ul>"},{"location":"changelog/#11-2023-06-18","title":"[1.1] - 2023-06-18","text":""},{"location":"changelog/#changed_14","title":"Changed","text":"<ul> <li>GCS chunk size lowered and timeout set to 120s for better network support</li> </ul>"},{"location":"changelog/#fixed_8","title":"Fixed","text":"<ul> <li>GCS tests after chunk_size changes</li> </ul>"},{"location":"changelog/#10-2023-05-28","title":"[1.0] - 2023-05-28","text":""},{"location":"changelog/#added_17","title":"Added","text":"<ul> <li>Comprehensive documentation</li> <li>License update</li> </ul>"},{"location":"changelog/#016-2023-05-28","title":"[0.16] - 2023-05-28","text":""},{"location":"changelog/#added_18","title":"Added","text":"<ul> <li>Configuration documentation</li> <li>How to restore documentation</li> <li>Deployment documentation</li> <li><code>env_name</code> added to backup file names</li> </ul>"},{"location":"changelog/#015-2023-05-25","title":"[0.15] - 2023-05-25","text":""},{"location":"changelog/#added_19","title":"Added","text":"<ul> <li>Added <code>.sql</code> postfix to backup files</li> </ul>"},{"location":"changelog/#changed_15","title":"Changed","text":"<ul> <li>BREAKING: PostgreSQL backups now use plain SQL format instead of <code>-Fc</code></li> </ul>"},{"location":"changelog/#fixed_9","title":"Fixed","text":"<ul> <li>Documentation updates for backup targets</li> </ul>"},{"location":"changelog/#014-2023-05-24","title":"[0.14] - 2023-05-24","text":""},{"location":"changelog/#added_20","title":"Added","text":"<ul> <li>Special characters support for MariaDB, MySQL, and PostgreSQL</li> </ul>"},{"location":"changelog/#changed_16","title":"Changed","text":"<ul> <li>BREAKING: Environment variable parsing changed from JSON to pgbouncer-like <code>key=value</code> syntax</li> </ul>"},{"location":"changelog/#013-2023-05-07","title":"[0.13] - 2023-05-07","text":""},{"location":"changelog/#fixed_10","title":"Fixed","text":"<ul> <li>Log files permissions in Docker container at <code>/var/log</code></li> </ul>"},{"location":"changelog/#012-2023-05-07","title":"[0.12] - 2023-05-07","text":""},{"location":"changelog/#changed_17","title":"Changed","text":"<ul> <li>Set <code>LOG_FOLDER_PATH</code> to <code>/var/log</code> in container</li> </ul>"},{"location":"changelog/#011-2023-05-07","title":"[0.11] - 2023-05-07","text":""},{"location":"changelog/#fixed_11","title":"Fixed","text":"<ul> <li>Typo in <code>_init_pgpass_file</code></li> </ul>"},{"location":"changelog/#010-2023-05-06","title":"[0.10] - 2023-05-06","text":""},{"location":"changelog/#changed_18","title":"Changed","text":"<ul> <li>PostgreSQL pgpass file initialization with different connection params</li> </ul>"},{"location":"changelog/#09-2023-05-06","title":"[0.9] - 2023-05-06","text":""},{"location":"changelog/#fixed_12","title":"Fixed","text":"<ul> <li>Valid venv path in Dockerfile</li> </ul>"},{"location":"changelog/#08-2023-05-06","title":"[0.8] - 2023-05-06","text":""},{"location":"changelog/#fixed_13","title":"Fixed","text":"<ul> <li>pgpass file permissions for <code>CONST_PGPASS_FILE_PATH</code></li> </ul>"},{"location":"changelog/#07-2023-05-04","title":"[0.7] - 2023-05-04","text":""},{"location":"changelog/#added_21","title":"Added","text":"<ul> <li>ARM64 architecture support and tests</li> <li>Documentation pages</li> <li>Base backup tests</li> <li>Graceful shutdown with daemon threads and timeout</li> <li><code>ZIP_ARCHIVE_LEVEL</code> environment variable</li> <li>Log rotation (max ~100KB per file)</li> </ul>"},{"location":"changelog/#06-2023-04-28","title":"[0.6] - 2023-04-28","text":""},{"location":"changelog/#changed_19","title":"Changed","text":"<ul> <li>MySQL client installation commented out in install script</li> </ul>"},{"location":"changelog/#05-2023-04-28","title":"[0.5] - 2023-04-28","text":""},{"location":"changelog/#added_22","title":"Added","text":"<ul> <li>MariaDB backup target support</li> <li>MySQL backup target support</li> <li>Single file and directory backup targets</li> <li>Discord webhook notifications</li> <li>Google Cloud Storage provider</li> <li>MkDocs documentation</li> <li>Validation for environment variables and paths</li> <li>Password regex validation</li> <li>ZIP archive password support</li> </ul>"},{"location":"changelog/#changed_20","title":"Changed","text":"<ul> <li>Renamed project to \"backuper\"</li> <li>Provider architecture improvements</li> </ul>"},{"location":"changelog/#040-2022-12-11","title":"[0.4.0] - 2022-12-11","text":""},{"location":"changelog/#added_23","title":"Added","text":"<ul> <li>Tests for core module</li> <li>Runtime arguments</li> <li>7zip precompiled binaries</li> <li>Retry logic for GCS provider</li> <li>Post-save and clean methods for providers</li> </ul>"},{"location":"changelog/#changed_21","title":"Changed","text":"<ul> <li>Dockerfile improvements with multi-stage builds</li> </ul>"},{"location":"changelog/#030-2022-08-20","title":"[0.3.0] - 2022-08-20","text":""},{"location":"changelog/#changed_22","title":"Changed","text":"<ul> <li>Updated Pydantic</li> <li>Improved Docker image to prevent immediate exit</li> <li>Reduced CPU usage with better sleep intervals</li> </ul>"},{"location":"changelog/#02-2022-08-20","title":"[0.2] - 2022-08-20","text":""},{"location":"changelog/#added_24","title":"Added","text":"<ul> <li>Expressive README</li> <li>Docker environment variables</li> </ul>"},{"location":"changelog/#01-2022-08-15","title":"[0.1] - 2022-08-15","text":""},{"location":"changelog/#added_25","title":"Added","text":"<ul> <li>Initial release</li> <li>PostgreSQL backup support with pg_dump</li> <li>Basic Docker support</li> <li>GPG encryption support</li> <li>Multi-threaded backup architecture</li> <li>Cron-like scheduling with croniter</li> <li>Environment variable configuration</li> </ul>"},{"location":"cli/","title":"CLI Reference","text":"<pre><code>usage: python3 -m ogion.main [-h] [-s] [-n]\n                             [--debug-download DEBUG_DOWNLOAD]\n                             [--debug-loop DEBUG_LOOP]\n                             [--target TARGET]\n                             [--restore-latest]\n                             [-r RESTORE] [-l]\n\nOgion - Automated database backup and secure cloud upload tool\n\noptions:\n  -h, --help            show this help message and exit\n  -s, --single          Run single backup then exit\n                        (optionally for specific --target)\n  -n, --debug-notifications\n                        Check if notifications setup is\n                        working\n  --debug-download DEBUG_DOWNLOAD\n                        Download given backup file locally\n                        and print path\n  --debug-loop DEBUG_LOOP\n                        Run N backup iterations ignoring\n                        cron schedule (for stress/memory\n                        testing)\n  --target TARGET       Backup target (required with\n                        --list, --restore-latest,\n                        --restore)\n  --restore-latest      Restore given target to latest\n                        backup\n  -r, --restore RESTORE\n                        Restore given target to specific\n                        backup file\n  -l, --list            List all backups for given target\n\nExamples:\n  ogion                                 Run in continuous backup mode\n  ogion -s                              Run a single backup for all targets\n  ogion --target mytarget -s            Run a single backup for specific target\n  ogion --target mytarget --list\n                                        List all backups for 'mytarget' target\n  ogion --target mytarget --restore-latest\n                                        Restore the latest backup for 'mytarget'\n  ogion --target mytarget --restore backup_file.sql.lz.age\n                                        Restore specific backup file for 'mytarget'\n</code></pre> <p>Note</p> <p>The <code>ogion</code> command is a bash script shortcut for <code>python -m ogion.main</code>. Both work and have autocompletion support.</p>"},{"location":"cli/#disaster-recovery","title":"Disaster Recovery","text":"<p>Get a bash shell in the container, eg.:</p> <p>Kubernetes: <pre><code>kubectl exec -it ogion-9c8b8b77d-z5xsc -n ogion -- bash\n</code></pre></p> <p>Docker: <pre><code>docker exec -it ogion bash\n</code></pre></p> <p>Then run restore commands:</p> <pre><code># Restore to latest backup\nogion --target postgresql_my-instance --restore-latest\n\n# Or first list available backups\nogion --target postgresql_my-instance --list\n\n# And restore to specific backup\nogion --target postgresql_my-instance --restore backup_file.sql.lz.age\n</code></pre> <p>Note</p> <p>You'll be prompted for the age secret key during restore.</p>"},{"location":"configuration/","title":"Configuration","text":"<p>Environemt variables</p> Name Type Description Default AGE_RECIPIENTS string[required] AGE public keys. Can be many splitted by comma. Note those must be public keys. Keep you private keys safe. - BACKUP_PROVIDER string[required] See <code>Providers</code> chapter, choosen backup provider for example GCS. - INSTANCE_NAME string Name of this ogion instance, will be used for example when sending fail messages. Defaults to system hostname. system hostname BACKUP_MAX_NUMBER int Soft limit how many backups can live at once for backup target. Defaults to <code>7</code>. This must makes sense with cron expression you use. For example if you want to have <code>7</code> day retention, and make backups at 5:00, <code>max_backups=7</code> is fine, but if you make <code>4</code> backups per day, you would need <code>max_backups=28</code>. Limit is soft and can be exceeded if no backup is older than value specified in <code>min_retention_days</code> in backup target. Note this global default and can be overwritten by using <code>max_backups</code> param in specific targets. Min <code>1</code> and max <code>998</code>. 7 BACKUP_MIN_RETENTION_DAYS int Hard minimum backups lifetime in days. Ogion won't ever delete files before, regardles of other options. Note this global default and can be overwritten by using <code>min_retention_days</code> param in specific targets. Min <code>0</code> and max <code>36600</code>. 3 BACKUP_DELETE bool Controls whether Ogion performs cleanup operations. When <code>true</code> (default), Ogion will automatically delete old backups from storage based on <code>max_backups</code> and <code>min_retention_days</code> settings. When <code>false</code>, Ogion only uploads backups without any cleanup, allowing external tools like GCS bucket expiry rules, S3 lifecycle policies, or Azure blob lifecycle management to handle deletion. Note: When disabled, cloud storage permissions can be reduced - you won't need delete or list permissions, only write/upload permissions are required. true POSTGRESQL_... backup target syntax PostgreSQL database target, see PostgreSQL. - MARIADB_... backup target syntax MariaDB database target, see MariaDB. - SINGLEFILE_... backup target syntax Single file database target, see Single file. - DIRECTORY_... backup target syntax Directory database target, see Directory. - LZIP_LEVEL int Compression level for LZIP (0-9). Higher values mean better compression but slower speed. 0 LZIP_THREADS int Number of threads for LZIP compression and decompression (1-1024). When not set, plzip automatically detects the number of CPU cores available and uses that as the default. Setting this value will override plzip's automatic detection. Note that the actual number of threads used may be lower depending on file size and available system resources. - DISCORD_WEBHOOK_URL http url Webhook URL for fail messages. - DISCORD_MAX_MSG_LEN int Maximum length of messages send to discord API. Sensible default used. Min <code>150</code> and max <code>10000</code>. 1500 SLACK_WEBHOOK_URL http url Webhook URL for fail messages. - SLACK_MAX_MSG_LEN int Maximum length of messages send to slack API. Sensible default used. Min <code>150</code> and max <code>10000</code>. 1500 SMTP_HOST string SMTP server host. - SMTP_FROM_ADDR string Email address that will send emails. - SMTP_PASSWORD string Password for <code>SMTP_FROM_ADDR</code>. - SMTP_TO_ADDRS string Comma separated list of email addresses to send emails. For example <code>email1@example.com,email2@example.com</code>. - SMTP_PORT int SMTP server port. 587 LOG_LEVEL string Case sensitive const log level, must be one of <code>INFO</code>, <code>DEBUG</code>, <code>WARNING</code>, <code>ERROR</code>, <code>CRITICAL</code>. INFO SUBPROCESS_TIMEOUT_SECS int Indicates how long subprocesses can last. Note that all backups are run from shell in subprocesses. Defaults to 3600 seconds which should be enough for even big dbs to make backup of. Min <code>5</code> and max <code>86400</code> (24h). 3600 SIGTERM_TIMEOUT_SECS int Time in seconds on exit how long ogion will wait for ongoing backup threads before force killing them and exiting. Min <code>0</code> and max <code>86400</code> (24h). 3600 OGION_CPU_ARCHITECTURE string CPU architecture, supported <code>amd64</code> and <code>arm64</code>. Docker container will set it automatically so probably do not change it. null DEBUG_AGE_SECRET_KEY string AGE single secret key used to automatically decrypt when using <code>--restore</code> or <code>--restore-latest</code> command without asking for it in input. Only for debug, tests or when you know what you are doing. amd64 <p> </p>"},{"location":"deployment/","title":"Deployment","text":"<p>Use docker image <code>rafsaf/ogion</code> (available tags on dockerhub). Supports both <code>amd64</code> and <code>arm64</code> architectures. Standard deployment methods are docker compose or kubernetes. Using <code>latest</code> tag is not recommended.</p> <p>For runtime flags and CLI commands, see CLI Reference. For environment variables and configuration, see Configuration.</p>"},{"location":"deployment/#docker-compose","title":"Docker Compose","text":"<pre><code># docker-compose.yml\n\nservices:\n  ogion:\n    container_name: ogion\n    image: rafsaf/ogion:8.2\n    network_mode: host\n    read_only: true\n    security_opt:\n      - no-new-privileges:true\n    cap_drop:\n      - ALL\n    volumes:\n      - ogion_data:/var/lib/ogion/data\n    env_file:\n      - .env\n\nvolumes:\n  ogion_data:\n</code></pre>"},{"location":"deployment/#kubernetes","title":"Kubernetes","text":"<pre><code># ogion-deployment.yml\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: ogion\n  name: ogion\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: ogion\n  template:\n    metadata:\n      labels:\n        app: ogion\n    spec:\n      securityContext:\n        runAsUser: 1000\n        runAsGroup: 1000\n        fsGroup: 1000\n        runAsNonRoot: true\n      containers:\n        - name: ogion\n          image: rafsaf/ogion:8.2\n          imagePullPolicy: Always\n          securityContext:\n            readOnlyRootFilesystem: true\n            allowPrivilegeEscalation: false\n            capabilities:\n              drop:\n                - ALL\n            seccompProfile:\n              type: RuntimeDefault\n          envFrom:\n            - secretRef:\n                name: ogion-secrets\n          env:\n            - name: LZIP_THREADS\n              valueFrom:\n                resourceFieldRef:\n                  resource: limits.cpu\n          resources:\n            requests:\n              cpu: \"20m\"\n              memory: \"512Mi\"\n            limits:\n              cpu: \"2\"\n              memory: \"512Mi\"\n          volumeMounts:\n            - name: data\n              mountPath: /var/lib/ogion/data\n      volumes:\n        - name: data\n          emptyDir: {}\n</code></pre>"},{"location":"recovery/","title":"Manual Recovery","text":"<p>Preferred method is using ogion itself - see CLI Reference.</p> <p>If you need to restore manually without ogion, follow the steps below.</p>"},{"location":"recovery/#recovery-process","title":"Recovery Process","text":""},{"location":"recovery/#1-download-backup","title":"1. Download Backup","text":"<p>Download the backup file from your cloud provider (GCS, S3, Azure).</p>"},{"location":"recovery/#2-decrypt-with-age","title":"2. Decrypt with age","text":"<p>You need the age private key that matches the public key used during backup.</p> <pre><code>age -d -i /path/to/key.txt -o backup.sql.lz backup.sql.lz.age\n</code></pre>"},{"location":"recovery/#3-decompress-with-lzip","title":"3. Decompress with lzip","text":"<pre><code>lzip -d backup.sql.lz\n</code></pre> <p>This produces the final backup file (e.g., <code>backup.sql</code>).</p>"},{"location":"recovery/#4-restore-based-on-type","title":"4. Restore Based on Type","text":""},{"location":"recovery/#postgresql","title":"PostgreSQL","text":"<p>Restore using <code>psql</code>:</p> <pre><code>psql -h localhost -p 5432 -U postgres -d database_name &lt; backup.sql\n</code></pre>"},{"location":"recovery/#mariadb","title":"MariaDB","text":"<p>Restore using <code>mariadb</code>:</p> <pre><code>mariadb -h localhost -P 3306 -u root -p database_name &lt; backup.sql\n</code></pre>"},{"location":"recovery/#file","title":"File","text":"<p>Copy the file back:</p> <pre><code>cp backup_file /destination/path\n</code></pre>"},{"location":"recovery/#directory","title":"Directory","text":"<p>Extract the tar archive:</p> <pre><code>tar xf backup.tar -C /destination/path --strip-components=1\n</code></pre>"},{"location":"backup_targets/directory/","title":"Directory","text":""},{"location":"backup_targets/directory/#environment-variable","title":"Environment variable","text":"<pre><code>DIRECTORY_SOME_STRING=\"abs_path=... cron_rule=...\"\n</code></pre> <p>Note</p> <p>Any environment variable that starts with \"DIRECTORY_\" will be handled as Directory. There can be multiple files paths definition for one ogion instance, for example <code>DIRECTORY_FOO</code> and <code>DIRECTORY_BAR</code>. Params must be included in value, splited by single space for example <code>\"value1=1 value2=foo\"</code>.</p>"},{"location":"backup_targets/directory/#params","title":"Params","text":"Name Type Description Default abs_path string[requried] Absolute path to folder for backup. - cron_rule string[requried] Cron expression for backups, see https://crontab.guru/ for help. - max_backups int Soft limit how many backups can live at once for backup target. Defaults to <code>7</code>. This must makes sense with cron expression you use. For example if you want to have <code>7</code> day retention, and make backups at 5:00, <code>max_backups=7</code> is fine, but if you make <code>4</code> backups per day, you would need <code>max_backups=28</code>. Limit is soft and can be exceeded if no backup is older than value specified in min_retention_days. Min <code>1</code> and max <code>998</code>. Defaults to enviornment variable BACKUP_MAX_NUMBER, see Configuration. BACKUP_MAX_NUMBER min_retention_days int Hard minimum backups lifetime in days. Ogion won't ever delete files before, regardles of other options. Min <code>0</code> and max <code>36600</code>. Defaults to enviornment variable BACKUP_MIN_RETENTION_DAYS, see Configuration. BACKUP_MIN_RETENTION_DAYS"},{"location":"backup_targets/directory/#examples","title":"Examples","text":"<pre><code># 1. Directory /home/user/folder with backup every single minute\nDIRECTORY_FIRST='abs_path=/home/user/folder cron_rule=* * * * *'\n\n# 2. Directory /etc with backup on every night (UTC) at 05:00\nDIRECTORY_SECOND='abs_path=/etc cron_rule=0 5 * * *'\n\n# 3. Mounted directory /mnt/homedir with backup on every 6 hours at '15 with max number of backups of 20\nDIRECTORY_HOME_DIR='abs_path=/mnt/homedir cron_rule=15 */3 * * * max_backups=20'\n</code></pre>"},{"location":"backup_targets/file/","title":"Single file","text":""},{"location":"backup_targets/file/#environment-variable","title":"Environment variable","text":"<pre><code>SINGLEFILE_SOME_STRING=\"abs_path=... cron_rule=...\"\n</code></pre> <p>Note</p> <p>Any environment variable that starts with \"SINGLEFILE_\" will be handled as Single File. There can be multiple files paths definition for one ogion instance, for example <code>SINGLEFILE_FOO</code> and <code>SINGLEFILE_BAR</code>. Params must be included in value, splited by single space for example <code>\"value1=1 value2=foo\"</code>.</p>"},{"location":"backup_targets/file/#params","title":"Params","text":"Name Type Description Default abs_path string[requried] Absolute path to file for backup. - cron_rule string[requried] Cron expression for backups, see https://crontab.guru/ for help. - max_backups int Soft limit how many backups can live at once for backup target. Defaults to <code>7</code>. This must makes sense with cron expression you use. For example if you want to have <code>7</code> day retention, and make backups at 5:00, <code>max_backups=7</code> is fine, but if you make <code>4</code> backups per day, you would need <code>max_backups=28</code>. Limit is soft and can be exceeded if no backup is older than value specified in min_retention_days. Min <code>1</code> and max <code>998</code>. Defaults to enviornment variable BACKUP_MAX_NUMBER, see Configuration. BACKUP_MAX_NUMBER min_retention_days int Hard minimum backups lifetime in days. Ogion won't ever delete files before, regardles of other options. Min <code>0</code> and max <code>36600</code>. Defaults to enviornment variable BACKUP_MIN_RETENTION_DAYS, see Configuration. BACKUP_MIN_RETENTION_DAYS"},{"location":"backup_targets/file/#examples","title":"Examples","text":"<pre><code># File /home/user/file.txt with backup every single minute\nSINGLEFILE_FIRST='abs_path=/home/user/file.txt cron_rule=* * * * *'\n\n# File /etc/hosts with backup on every night (UTC) at 05:00\nSINGLEFILE_SECOND='abs_path=/etc/hosts cron_rule=0 5 * * *'\n\n# File config.json in mounted dir /mnt/appname with backup on every 6 hours at '15 with max number of backups of 20\nSINGLEFILE_THIRD='abs_path=/mnt/appname/config.json cron_rule=15 */3 * * * max_backups=20'\n</code></pre>"},{"location":"backup_targets/mariadb/","title":"MariaDB","text":""},{"location":"backup_targets/mariadb/#environment-variable","title":"Environment variable","text":"<pre><code>MARIADB_SOME_STRING=\"host=... password=... cron_rule=...\"\n</code></pre> <p>Note</p> <p>Any environment variable that starts with \"MARIADB_\" will be handled as MariaDB. There can be multiple files paths definition for one ogion instance, for example <code>MARIADB_FOO_MY_DB1</code> and <code>MARIADB_BAR_MY_DB2</code>. All currently supported versions are also supported by ogion. Params must be included in value, splited by single space for example <code>\"value1=1 value2=foo\"</code>.</p>"},{"location":"backup_targets/mariadb/#params","title":"Params","text":"Name Type Description Default password string[requried] Mariadb database password. - cron_rule string[requried] Cron expression for backups, see https://crontab.guru/ for help. - user string Mariadb database username. root host string Mariadb database hostname. localhost port int Mariadb database port. 3306 db string Mariadb database name. mariadb max_backups int Soft limit how many backups can live at once for backup target. Defaults to <code>7</code>. This must makes sense with cron expression you use. For example if you want to have <code>7</code> day retention, and make backups at 5:00, <code>max_backups=7</code> is fine, but if you make <code>4</code> backups per day, you would need <code>max_backups=28</code>. Limit is soft and can be exceeded if no backup is older than value specified in min_retention_days. Min <code>1</code> and max <code>998</code>. Defaults to enviornment variable BACKUP_MAX_NUMBER, see Configuration. BACKUP_MAX_NUMBER min_retention_days int Hard minimum backups lifetime in days. Ogion won't ever delete files before, regardles of other options. Min <code>0</code> and max <code>36600</code>. Defaults to enviornment variable BACKUP_MIN_RETENTION_DAYS, see Configuration. BACKUP_MIN_RETENTION_DAYS"},{"location":"backup_targets/mariadb/#additional-connection-client-params","title":"Additional connection client params","text":"<p>Extra variables that starts with <code>client_</code> will be passed AS IS to mariadb command underthehood as escaped lines in client section of <code>.cnf</code> configuration file, see https://mariadb.com/kb/en/mariadb-command-line-client:</p> <p>For example you can use it for SSL setup:</p> <ul> <li><code>client_ssl=true</code></li> <li><code>client_ssl-ca=path1</code></li> <li><code>client_ssl-cert=path2</code></li> <li><code>client_ssl-key=path3</code></li> </ul>"},{"location":"backup_targets/mariadb/#examples","title":"Examples","text":"<pre><code># 1. Local MariaDB with backup every single minute\nMARIADB_FIRST_DB='host=localhost port=3306 password=secret cron_rule=* * * * *'\n\n# 2. MariaDB in local network with backup on every night (UTC) at 05:00\nMARIADB_SECOND_DB='host=10.0.0.1 port=3306 user=foo password=change_me! db=bar cron_rule=0 5 * * *'\n\n# 3. MariaDB in local network with backup on every 6 hours at '15 with max number of backups of 20\nMARIADB_THIRD_DB='host=192.168.1.5 port=3306 user=root password=change_me_please! db=project cron_rule=15 */3 * * * max_backups=20'\n\n# 4. MariaDB local database above 11.3 with ssl-verify-server-cert disabled\nMARIADB_4_DB='host=localhost port=3306 password=secret cron_rule=* * * * * client_ssl-verify-server-cert=false'\n\n# 5. MariaDB with ssl disabled using skip-ssl\nMARIADB_5_DB='host=localhost port=3306 password=secret cron_rule=* * * * * client_skip-ssl=true'\n</code></pre>"},{"location":"backup_targets/mysql/","title":"MySQL","text":"<p>Please use MariaDB backup target for MySQL. It's fully compatibile.</p> <p> </p>"},{"location":"backup_targets/postgresql/","title":"PostgreSQL","text":""},{"location":"backup_targets/postgresql/#environment-variable","title":"Environment variable","text":"<pre><code>POSTGRESQL_SOME_STRING=\"host=... password=... cron_rule=...\"\n</code></pre> <p>Note</p> <p>Any environment variable that starts with \"POSTGRESQL_\" will be handled as PostgreSQL. There can be multiple files paths definition for one ogion instance, for example <code>POSTGRESQL_FOO_MY_DB1</code> and <code>POSTGRESQL_BAR_MY_DB2</code>. All currently supported versions are also supported by ogion. Changes in versions are automatically tracke . Params must be included in value, splited by single space for example <code>\"value1=1 value2=foo\"</code>.</p>"},{"location":"backup_targets/postgresql/#params","title":"Params","text":"Name Type Description Default password string[requried] PostgreSQL database password. - cron_rule string[requried] Cron expression for backups, see https://crontab.guru/ for help. - user string PostgreSQL database username. postgres host string PostgreSQL database hostname. localhost port int PostgreSQL database port. 5432 db string PostgreSQL database name. postgres max_backups int Soft limit how many backups can live at once for backup target. Defaults to <code>7</code>. This must makes sense with cron expression you use. For example if you want to have <code>7</code> day retention, and make backups at 5:00, <code>max_backups=7</code> is fine, but if you make <code>4</code> backups per day, you would need <code>max_backups=28</code>. Limit is soft and can be exceeded if no backup is older than value specified in min_retention_days. Min <code>1</code> and max <code>998</code>. Defaults to enviornment variable BACKUP_MAX_NUMBER, see Configuration. BACKUP_MAX_NUMBER min_retention_days int Hard minimum backups lifetime in days. Ogion won't ever delete files before, regardles of other options. Min <code>0</code> and max <code>36600</code>. Defaults to enviornment variable BACKUP_MIN_RETENTION_DAYS, see Configuration. BACKUP_MIN_RETENTION_DAYS"},{"location":"backup_targets/postgresql/#additional-connection-params","title":"Additional connection params","text":"<p>Extra variables that starts with <code>conn_</code> will be passed AS IS to psql command underthehood as url-encoded connection params:</p> <p>For example you can use it for SSL setup:</p> <ul> <li><code>conn_sslmode=verify-ca</code></li> <li><code>conn_sslrootcert=path-to-mounted-server-ca-file</code></li> <li><code>conn_sslcert=path-to-mounted-client-ca-file</code></li> <li><code>conn_sslkey=path-to-mounted-client-key-file</code></li> </ul>"},{"location":"backup_targets/postgresql/#examples","title":"Examples","text":"<pre><code># 1. Local PostgreSQL with backup every single minute\nPOSTGRESQL_FIRST_DB='host=localhost port=5432 password=secret cron_rule=* * * * *'\n\n# 2. PostgreSQL in local network with backup on every night (UTC) at 05:00\nPOSTGRESQL_SECOND_DB='host=10.0.0.1 port=5432 user=foo password=change_me! db=bar cron_rule=0 5 * * *'\n\n# 3. PostgreSQL in local network with backup on every 6 hours at '15 with max number of backups of 20\nPOSTGRESQL_THIRD_DB='host=192.168.1.5 port=5432 user=root password=change_me_please! db=project cron_rule=15 */3 * * * max_backups=20'\n\n# 4. PostgreSQL connected using sslmode require\nPOSTGRESQL_4_DB_SSL='host=localhost port=5432 password=secret cron_rule=* * * * * conn_sslmode=require'\n</code></pre>"},{"location":"notifications/discord/","title":"Discord","text":"<p>It is possible to send messages to your Discord channels in events of failed backups.</p> <p>Integration is via Discord webhooks and environment variables.</p> <p>Follow their documentation https://support.discord.com/hc/en-us/articles/228383668-Intro-to-Webhooks.</p> <p>You should be able to generate webhooks like <code>\"https://discord.com/api/webhooks/1111111111/some-long-token\"</code>.</p>"},{"location":"notifications/discord/#environemt-variables","title":"Environemt variables","text":"Name Type Description Default DISCORD_WEBHOOK_URL http url Webhook URL for fail messages. - DISCORD_MAX_MSG_LEN int Maximum length of messages send to discord API. Sensible default used. Min <code>150</code> and max <code>10000</code>. 1500"},{"location":"notifications/discord/#examples","title":"Examples:","text":"<pre><code>DISCORD_WEBHOOK_URL=\"https://discord.com/api/webhooks/1111111111/long-token\"\n</code></pre>"},{"location":"notifications/slack/","title":"Slack","text":"<p>It is possible to send messages to your Slack channels in events of failed backups.</p> <p>Integration is via Slack webhooks and environment variables.</p> <p>Follow their documentation https://api.slack.com/messaging/webhooks#create_a_webhook.</p> <p>You should be able to generate webhooks like <code>\"https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX\"</code>.</p>"},{"location":"notifications/slack/#environemt-variables","title":"Environemt variables","text":"Name Type Description Default SLACK_WEBHOOK_URL http url Webhook URL for fail messages. - SLACK_MAX_MSG_LEN int Maximum length of messages send to slack API. Sensible default used. Min <code>150</code> and max <code>10000</code>. 1500"},{"location":"notifications/slack/#examples","title":"Examples:","text":"<pre><code>SLACK_WEBHOOK_URL=\"https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX\"\n</code></pre>"},{"location":"notifications/smtp/","title":"Email (SMTP)","text":"<p>It is possible to send messages via email using SMTP protocol. Implementation uses STARTTLS so be sure you mail server support it. For technical details refer to https://docs.python.org/3/library/smtplib.html.</p> <p>Note, when any of params <code>SMTP_HOST</code>, <code>SMTP_FROM_ADDR</code>, <code>SMTP_PASSWORD</code>, <code>SMTP_TO_ADDRS</code>  is set, all are required. If not provided, execption will be raised.</p>"},{"location":"notifications/smtp/#environemt-variables","title":"Environemt variables","text":"Name Type Description Default SMTP_HOST string[required] SMTP server host. - SMTP_FROM_ADDR string[required] Email address that will send emails. - SMTP_PASSWORD string[required] Password for <code>SMTP_FROM_ADDR</code>. - SMTP_TO_ADDRS string[required] Comma separated list of email addresses to send emails. For example <code>email1@example.com,email2@example.com</code>. - SMTP_PORT int SMTP server port. 587"},{"location":"notifications/smtp/#examples","title":"Examples:","text":"<pre><code>SMTP_HOST=\"pro2.mail.ovh.net\"\nSMTP_FROM_ADDR=\"test@example.com\"\nSMTP_PASSWORD=\"changeme\"\nSMTP_TO_ADDRS=\"me@example.com,other@example.com\"\nSMTP_PORT=587\n</code></pre>"},{"location":"providers/azure/","title":"Azure Blob Storage","text":""},{"location":"providers/azure/#environment-variable","title":"Environment variable","text":"<pre><code>BACKUP_PROVIDER=\"name=azure container_name=my-ogion-instance connect_string=DefaultEndpointsProtocol=https;AccountName=accname;AccountKey=secret;EndpointSuffix=core.windows.net\"\n</code></pre> <p>Uses Azure Blob Storage for storing backups.</p> <p>Note</p> <p>There can be only one upload provider defined per app, using BACKUP_PROVIDER environemnt variable. It's type is guessed by using <code>name</code>, in this case <code>name=azure</code>. Params must be included in value, splited by single space for example \"value1=1 value2=foo\".</p>"},{"location":"providers/azure/#params","title":"Params","text":"Name Type Description Default name string[requried] Must be set literaly to string <code>azure</code> to use Azure. - container_name string[requried] Storage account container name. It must be already created, ogion won't create new container. - connect_string string[requried] Connection string copied from your storage account \"Access keys\" section. -"},{"location":"providers/azure/#examples","title":"Examples","text":"<pre><code># 1. Storage account accname and container name my-ogion-instance\nBACKUP_PROVIDER=\"name=azure container_name=my-ogion-instance connect_string=DefaultEndpointsProtocol=https;AccountName=accname;AccountKey=secret;EndpointSuffix=core.windows.net\"\n\n# 2. Storage account birds and container name birds\nBACKUP_PROVIDER=\"name=azure container_name=birds connect_string=DefaultEndpointsProtocol=https;AccountName=birds;AccountKey=secret;EndpointSuffix=core.windows.net\"\n</code></pre>"},{"location":"providers/azure/#resources","title":"Resources","text":""},{"location":"providers/azure/#creating-azure-storage-account","title":"Creating azure storage account","text":"<p>https://learn.microsoft.com/en-us/azure/storage/common/storage-account-create?tabs=azure-portal</p> <p>Reduced Permissions with BACKUP_DELETE=false</p> <p>If you set <code>BACKUP_DELETE=false</code> in your configuration, Ogion will only upload backups without performing cleanup operations. The built-in Storage Blob Data Contributor role includes read, write, AND delete permissions. For minimum permissions with BACKUP_DELETE=false, create a custom RBAC role with only <code>Microsoft.Storage/storageAccounts/blobServices/containers/blobs/write</code> permission. Alternatively, connection strings bypass RBAC and work with container-level access. This is useful when using Azure Blob Storage lifecycle management policies to handle backup expiration.</p> <p> </p>"},{"location":"providers/debug/","title":"Debug","text":""},{"location":"providers/debug/#environment-variable","title":"Environment variable","text":"<pre><code>BACKUP_PROVIDER=\"name=debug\"\n</code></pre> <p>Uses only local files (folder inside container) for storing backup. This is meant only for debug purposes.</p> <p>If you absolutely must not upload backups to outside world, consider adding some persistant volume for folder where buckups live in the container, that is <code>/var/lib/ogion/data</code>.</p> <p>Note</p> <p>There can be only one upload provider defined per app, using BACKUP_PROVIDER environemnt variable. It's type is guessed by using <code>name</code>, in this case <code>name=debug</code>.</p>"},{"location":"providers/debug/#params","title":"Params","text":"Name Type Description Default name string[requried] Must be set literaly to string <code>debug</code> to use Debug. -"},{"location":"providers/debug/#examples","title":"Examples","text":"<pre><code># 1. Debug provider\nBACKUP_PROVIDER='name=debug'\n</code></pre>"},{"location":"providers/google_cloud_storage/","title":"Google Cloud Storage","text":""},{"location":"providers/google_cloud_storage/#environment-variable","title":"Environment variable","text":"<pre><code>BACKUP_PROVIDER=\"name=gcs bucket_name=my_bucket_name bucket_upload_path=my_ogion_instance_1 service_account_base64=Z29vZ2xlX3NlcnZpY2VfYWNjb3VudAo=\"\n</code></pre> <p>Uses Google Cloud Storage bucket for storing backups.</p> <p>Note</p> <p>There can be only one upload provider defined per app, using BACKUP_PROVIDER environemnt variable. It's type is guessed by using <code>name</code>, in this case <code>name=gcs</code>. Params must be included in value, splited by single space for example \"value1=1 value2=foo\".</p>"},{"location":"providers/google_cloud_storage/#params","title":"Params","text":"Name Type Description Default name string[requried] Must be set literaly to string <code>gcs</code> to use Google Cloud Storage. - bucket_name string[requried] Your globally unique bucket name. - bucket_upload_path string[requried] Prefix that every created backup will have, for example if it is equal to <code>my_ogion_instance_1</code>, paths to backups will look like <code>my_ogion_instance_1/your_backup_target_eg_postgresql/file123.age</code>. Usually this should be something unique for this ogion instance, for example <code>k8s_foo_ogion</code>. - service_account_base64 string[requried] Base64 JSON service account file created in IAM, with write and read access permissions to bucket, see Resources below. - chunk_size_mb int The size of a chunk of data transfered to GCS, consider lower value only if for example your internet connection is slow or you know what you are doing, 100MB is google default. 100 chunk_timeout_secs int The chunk of data transfered to GCS upload timeout, consider higher value only if for example your internet connection is slow or you know what you are doing, 60s is google default. 60"},{"location":"providers/google_cloud_storage/#examples","title":"Examples","text":"<pre><code># 1. Bucket pets-bucket\nBACKUP_PROVIDER='name=gcs bucket_name=pets-bucket bucket_upload_path=pets_ogion service_account_base64=Z29vZ2xlX3NlcnZpY2VfYWNjb3VudAo='\n\n# 2. Bucket birds with smaller chunk size\nBACKUP_PROVIDER='name=gcs bucket_name=birds bucket_upload_path=birds_ogion chunk_size_mb=25 chunk_timeout_secs=120 service_account_base64=Z29vZ2xlX3NlcnZpY2VfYWNjb3VudAo='\n</code></pre>"},{"location":"providers/google_cloud_storage/#resources","title":"Resources","text":""},{"location":"providers/google_cloud_storage/#creating-bucket","title":"Creating bucket","text":"<p>https://cloud.google.com/storage/docs/creating-buckets</p>"},{"location":"providers/google_cloud_storage/#creating-service-account","title":"Creating service account","text":"<p>https://cloud.google.com/iam/docs/service-accounts-create</p>"},{"location":"providers/google_cloud_storage/#giving-it-required-roles-to-service-account","title":"Giving it required roles to service account","text":"<ol> <li> <p>Go \"IAM and admin\" -&gt; \"IAM\"</p> </li> <li> <p>Find your service account and update its roles</p> </li> </ol> <p>Give it following roles so it will have read access for whole bucket \"my_bucket_name\" and admin access for only path prefix \"my_ogion_instance_1\" in bucket \"my_bucket_name\":</p> <ol> <li>Storage Object Admin (with IAM condition: NAME starts with <code>projects/_/buckets/my_bucket_name/objects/my_ogion_instance_1</code>)</li> <li>Storage Object Viewer (with IAM condition: NAME starts with <code>projects/_/buckets/my_bucket_name</code>)</li> </ol> <p>Reduced Permissions with BACKUP_DELETE=false</p> <p>If you set <code>BACKUP_DELETE=false</code> in your configuration, Ogion will only upload backups without performing cleanup operations. In this case, you can use reduced permissions - only Storage Object Creator role is needed (no delete or list permissions required). This is useful when using GCS bucket lifecycle rules to manage backup expiration instead of Ogion's built-in cleanup.</p> <p>After sucessfully creating service account, create new private key with JSON type and download it. File similar to <code>your_project_name-03189413be28.json</code> will appear in your Downloads.</p> <p>To get base64 (without any new lines) from it, use command:</p> <pre><code>cat your_project_name-03189413be28.json | base64 -w 0\n</code></pre>"},{"location":"providers/google_cloud_storage/#terraform","title":"Terraform","text":"<p>If using terraform for managing cloud infra, Service Accounts definition can be following:</p> <pre><code>resource \"google_service_account\" \"ogion-my_ogion_instance_1\" {\n  account_id   = \"ogion-my_ogion_instance_1\"\n  display_name = \"SA my_ogion_instance_1 for ogion bucket access\"\n}\n\nresource \"google_project_iam_member\" \"ogion-my_ogion_instance_1-iam-object-admin\" {\n  project = local.project_id\n  role    = \"roles/storage.objectAdmin\"\n  member  = \"serviceAccount:${google_service_account.ogion-my_ogion_instance_1.email}\"\n  condition {\n    title      = \"object_admin_only_ogion_bucket_specific_path\"\n    expression = \"resource.name.startsWith(\\\"projects/_/buckets/my_bucket_name/objects/my_ogion_instance_1\\\")\"\n  }\n}\nresource \"google_project_iam_member\" \"ogion-my_ogion_instance_1-iam-object-viewer\" {\n  project = local.project_id\n  role    = \"roles/storage.objectViewer\"\n  member  = \"serviceAccount:${google_service_account.ogion-my_ogion_instance_1.email}\"\n\n  condition {\n    title      = \"object_viewer_only_ogion_bucket\"\n    expression = \"resource.name.startsWith(\\\"projects/_/buckets/my_bucket_name\\\")\"\n  }\n}\n</code></pre> <p> </p>"},{"location":"providers/s3/","title":"S3","text":""},{"location":"providers/s3/#environment-variable","title":"Environment variable","text":"<pre><code>BACKUP_PROVIDER=\"name=s3 bucket_name=my_bucket_name bucket_upload_path=my_ogion_instance_1 access_key=AKIAU5JB5UQDL8C3K6UP secret_key=nFTXlO7nsPNNUj59tFE21Py9tOO8fwOtHNsr3YwN region=eu-central-1\"\n</code></pre> <p>Uses S3 bucket for storing backups (by default AWS but own instance can be specified eg. Minio).</p> <p>Note</p> <p>There can be only one upload provider defined per app, using BACKUP_PROVIDER environemnt variable. It's type is guessed by using <code>name</code>, in this case <code>name=s3</code>. Params must be included in value, splited by single space for example \"value1=1 value2=foo\".</p>"},{"location":"providers/s3/#params","title":"Params","text":"Name Type Description Default name string[requried] Must be set literaly to string <code>s3</code> to use S3. - bucket_name string[requried] Your globally unique bucket name. - bucket_upload_path string[requried] Prefix that every created backup will have, for example if it is equal to <code>my_ogion_instance_1</code>, paths to backups will look like <code>my_ogion_instance_1/your_backup_target_eg_postgresql/file123.age</code>. Usually this should be something unique for this ogion instance, for example <code>k8s_foo_ogion</code>. - endpoint string S3 endpoint. s3.amazonaws.com secure string If set to <code>false</code>, connect to endpoint under http. true region string Bucket region. null access_key string User access key id, see Resources below. null secret_key string User access key secret, see Resources below. null"},{"location":"providers/s3/#examples","title":"Examples","text":"<pre><code># 1. AWS Bucket pets-bucket\nBACKUP_PROVIDER='name=s3 bucket_name=pets-bucket bucket_upload_path=pets_ogion access_key=AKIAU5JB5UQDL8C3K6UP secret_key=nFTXlO7nsPNNUj59tFE21Py9tOO8fwOtHNsr3YwN region=eu-central-1'\n\n# 2. AWS Bucket birds with other region\nBACKUP_PROVIDER='name=s3 bucket_name=birds bucket_upload_path=birds_ogion access_key=AKIAU5JB5UQDL8C3K6UP secret_key=nFTXlO7nsPNNUj59tFE21Py9tOO8fwOtHNsr3YwN region=us-east-1'\n\n# 3. Min.io instance \nBACKUP_PROVIDER='name=s3 endpoint=my-min.io.com bucket_name=pets-bucket bucket_upload_path=pets_ogion access_key=AKIAU5JB5UQDL8C3K6UP secret_key=nFTXlO7nsPNNUj59tFE21Py9tOO8fwOtHNsr3YwN region=default'\n\n# 4. Min.io localhost instance under http and no auth\nBACKUP_PROVIDER='name=s3 endpoint=localhost:9000 bucket_name=pets-bucket bucket_upload_path=pets_ogion secure=false'\n</code></pre>"},{"location":"providers/s3/#resources","title":"Resources","text":""},{"location":"providers/s3/#bucket-and-iam-walkthrough","title":"Bucket and IAM walkthrough","text":"<p>https://docs.aws.amazon.com/AmazonS3/latest/userguide/walkthrough1.html</p>"},{"location":"providers/s3/#giving-iam-user-required-permissions","title":"Giving IAM user required permissions","text":"<p>Assuming your bucket name is <code>my_bucket_name</code> and upload path <code>test-upload-path</code>, 3 permissions are needed for IAM user (s3:ListBucket, s3:PutObject, s3:DeleteObject):</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"AllowList\",\n      \"Effect\": \"Allow\",\n      \"Action\": \"s3:ListBucket\",\n      \"Resource\": \"arn:aws:s3:::my_bucket_name\",\n      \"Condition\": {\n        \"StringLike\": {\n          \"s3:prefix\": \"test-upload-path/*\"\n        }\n      }\n    },\n    {\n      \"Sid\": \"AllowPutGetDelete\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\"s3:PutObject\", \"s3:DeleteObject\"],\n      \"Resource\": \"arn:aws:s3:::my_bucket_name/test-upload-path/*\"\n    }\n  ]\n}\n</code></pre> <p>Reduced Permissions with BACKUP_DELETE=false</p> <p>If you set <code>BACKUP_DELETE=false</code> in your configuration, Ogion will only upload backups without performing cleanup operations. In this case, you can use reduced permissions - only <code>s3:PutObject</code> action is needed (no <code>s3:ListBucket</code> or <code>s3:DeleteObject</code> required). This is useful when using S3 lifecycle policies to manage backup expiration instead of Ogion's built-in cleanup:</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"AllowPutOnly\",\n      \"Effect\": \"Allow\",\n      \"Action\": \"s3:PutObject\",\n      \"Resource\": \"arn:aws:s3:::my_bucket_name/test-upload-path/*\"\n    }\n  ]\n}\n</code></pre> <p> </p>"}]}